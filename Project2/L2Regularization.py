# Project 2: Linear and Polynomial Regression
# Extra Credit: Polynomial Regression with Regularization
# Author: Duong Hoang
# CS 460G - 001
# Due Date: March 6th, 2022

'''
    Purpose: predict the synthetic data label using 5th order Polynomial Regression 
            with L2 Regularization and plot regression lines of each model
    Pre-cond: a synthetic data csv file
    Post-cond: predictions generated by the 5th order polynomial regression with 
            full batch gradient descent and L2 regularization; 
            the mean squared error averaged over the number of examples;
            the plot of data with the model's regression line

'''

### Implementation ###


# initialize
DATA_FILE = 'synthetic-1'
ALPHA = 0.01               # learning rate
NUM_EPOCHS = 2000           # iterations
ORDERS = 5                  # polynomial orders
LAMBDA = 0.0003                # penalty

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


class Regression():
    def __init__(self, X: pd.DataFrame, Y: pd.DataFrame, orders: int, 
                learning_rate: float, L2_norm: float, epochs: int):
        self.X = X              # features
        self.Y = Y              # classlabel
        self.orders = orders    # polynomial order
        self.expanded_X = self.basis_expansion(X) # expanded features
        self.thetas = self.coefficients(self.expanded_X, Y.to_numpy(),
                        learning_rate, L2_norm, epochs) # vector of coefficients
            
    
    def basis_expansion(self, X: pd.DataFrame):
        '''Return examples with expanded feature values using basis expansion'''
        
        expanded_X = X.to_numpy().copy()
        # expand feature values by number of orders
        for order in range(2, self.orders + 1):
            expanded_X = np.append(expanded_X, np.power(X, order), axis=1)

        return expanded_X


    def coefficients(self, X: np.matrix, Y: np.matrix, alpha: float, Lambda: float, epochs: int):
        '''Return a vector of thetas using full batch update gradient descent'''
        
        # add the bias feature x_0 = 1 to examples
        X = np.insert(X, 0, [1] * Y.shape[0], axis=1) # X is a [m x n] matrix
        
        # initialize a list of thetas with random values
        m = X.shape[0] # batch size m = number of examples
        thetas = np.random.uniform(-3.0, 3.0, size=(X.shape[1], 1)) # [n x 1] matrix

        for epoch in range(epochs):
            # calculate h_theta =  sigma(theta_j * x_i)
            h_thetas = X.dot(thetas) # [m x n] [n x 1] = [m x 1]
            # calculate errors = predicted - expected = h_theta_i - y_i
            errors = np.subtract(h_thetas, Y) # [m x 1] - [m x 1] = [m x 1]
            # calculate update weight = (alpha * gradient) / m + lambda * theta_j,
            # with gradient = error_i * x_i
            updates = (alpha / m) * X.transpose().dot(errors) + Lambda * thetas # [n x m] [m x 1] = [n x 1]
            # update theta_j = theta_j - weight
            thetas -= updates # [n x 1] - [n x 1] = [n x 1]

        return thetas


    def polynomial_regression(self, X: pd.DataFrame):
        '''Return predictions of polynomial regression model'''

        # expand data
        expanded_X = self.basis_expansion(X)
        # add the bias feature x_0 = 1 to examples
        expanded_X = np.insert(expanded_X, 0, [1] * expanded_X.shape[0], axis=1) 
        # get model predictions h_theta = theta_n * x_n
        predictions = expanded_X.dot(self.thetas) 

        return predictions


    def calculate_MSE(self, X: pd.DataFrame, Y: pd.DataFrame):
        '''Return the mean squared error of the model averaged over 
        the number of examples in the dataset
        '''
        
        m = len(Y) # number of examples        
        predictions = self.polynomial_regression(X) # linear regressor's predictions

        # create csv file with first column is prediction, second column is key
        df_predictions = pd.DataFrame(predictions)
        compare = pd.concat([df_predictions, Y], axis=1, join='inner')
        compare.to_csv(path_or_buf=f'classified_L2-norm_{DATA_FILE}_{self.orders}-order.csv', index=False)

        # calculate MSE: J_theta = 1/m * sigma((h_theta_n - y_n)**2) 
        J_theta = 1 / m * np.sum(np.square(np.subtract(predictions, Y.to_numpy())))
        
        return J_theta

    
    def visualize_model(self):
        '''Scatterplot visualization of examples and polynomial regression line'''

        # get regression model
        x = np.linspace(np.floor(self.X.min()), np.ceil(self.X.max()), 100)
        y = (self.thetas[0] + self.thetas[1] * x + self.thetas[2] * x**2 
            + self.thetas[3] * x**3 + self.thetas[4] * x**4 + self.thetas[5] * x**5)
        

        # create a scatter plot of examples
        plt.scatter(x=self.X, y=self.Y, color='blue')
        plt.plot(x, y, color='red')
        plt.title(f'{DATA_FILE} L2-norm {self.orders}-order Visualization')
        plt.xlabel('Feature Value')
        plt.ylabel('Class Label')

        #plt.margins(x=0, y=0) # eliminate whitespace surround plot
        plt.savefig(f'visualize_L2-norm_{self.orders}-order_{DATA_FILE}.png', bbox_inches='tight')
        plt.show() # display plot and export to file


def main():
    # read data
    data = pd.read_csv(f'{DATA_FILE}.csv')   
    label_index = len(data.columns) - 1
    X = pd.DataFrame(data.iloc[:, :label_index])    # features
    Y = pd.DataFrame(data.iloc[:, label_index])     # classlabel

    # create polynomial regression model 
    model = Regression(X, Y, ORDERS, ALPHA, LAMBDA, NUM_EPOCHS)
   
    # output weights of regression model
    print('Weights:', model.thetas)
    # output mean squared error
    print('Mean Squared Error =', model.calculate_MSE(X, Y))
    # draw graph with regression line
    model.visualize_model()
    print()

main()