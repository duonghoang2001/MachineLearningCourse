# Project 2: Linear and Polynomial Regression
# Part 1: Linear Regression with Gradient Descent
# Author: Duong Hoang
# CS 460G - 001
# Due Date: March 6th, 2022

'''
    Purpose: predict the quality of red wine based on 11 features describing 
            a particular vintage using linear regression with gradient descent
    Pre-cond: a wine quality data csv file
    Post-cond: predictions generated by the multiple linear regression  
                with full batch gradient descent and its corresponding 
                mean squared error averaged over the number of examples

'''

### Implementation ###


# initialize
DATA_FILE = 'winequality-red'
ALPHA = 0.01                    # learning rate
NUM_EPOCHS = 3000               # iterations

# import libraries
import pandas as pd
import numpy as np


class Regression():
    def __init__(self, X: pd.DataFrame, Y: pd.DataFrame, learning_rate: float, epochs: int):
        self.X = X # features
        self.Y = Y # classlabel
        self.norm_X = self.normalize_data(X) # normalized features (range 0-1)
        self.thetas = self.coefficients(self.norm_X.to_numpy(), Y.to_numpy(), 
                                        learning_rate, epochs) # vector of coefficients


    def find_limits(self, data: pd.DataFrame):
        '''Return a list of [min, max] pair of each columns'''

        limits = []
        for col in range(len(data.columns)):
            limits.append([min(data.iloc[:, col]), max(data.iloc[:, col])])

        return limits
            
            
    def normalize_data(self, data: pd.DataFrame):
        '''Normalize feature values to range 0-1'''

        normalized_data = data.copy()
        # get [min, max] values of each column
        limits = self.find_limits(data)

        # for each entry in the dataframe
        for row in range(len(data)):
            for col in range(len(data.columns)):
                # x_i = (x_i - min(x)) / (max(x) - min(x))
                normalized_data.iat[row, col] = (normalized_data.iat[row, col] 
                            - limits[col][0]) / (limits[col][1] - limits[col][0])

        return normalized_data


    def coefficients(self, X: np.matrix, Y: np.matrix, alpha: float, epochs: int):
        '''Return a vector of thetas using full batch update gradient descent'''

        # add the bias feature x_0 = 1 to examples
        X = np.insert(X, 0, [1] * Y.shape[0], axis=1) # X is a [m x n] matrix
        # initialize a list of thetas with random values
        m = X.shape[0] # batch size m = number of examples
        thetas = np.random.uniform(size=(X.shape[1], 1)) # [n x 1] matrix

        for epoch in range(epochs):
            # calculate h_theta =  sigma(theta_i * x_i)
            h_thetas = X.dot(thetas) # [m x n] [n x 1] = [m x 1]
            # calculate errors = predicted - expected = h_theta_i - y_i
            errors = np.subtract(h_thetas, Y) # [m x 1] - [m x 1] = [m x 1]
            # calculate update weight = (alpha * gradient) / m,
            # with gradient = error_i * x_i
            updates = (alpha / m) * X.transpose().dot(errors) # [n x m] [m x 1] = [n x 1]
            # update theta_i = theta_i - weight
            thetas -= updates # [n x 1] - [n x 1] = [n x 1]

        return thetas


    def linear_regression(self, X: pd.DataFrame):
        '''Return predictions of linear regression model'''

        # normalize data
        norm_X = self.normalize_data(X).to_numpy()
        # add the bias feature x_0 = 1 to examples
        norm_X = np.insert(norm_X, 0, [1] * norm_X.shape[0], axis=1) 
        # get model predictions h_theta = theta_n * x_n
        predictions = norm_X.dot(self.thetas) 

        return predictions


    def calculate_MSE(self, X: pd.DataFrame, Y: pd.DataFrame):
        '''Return the mean squared error of the model averaged over 
        the number of examples in the dataset
        '''
        
        m = len(Y) # number of examples        
        predictions = self.linear_regression(X) # linear regressor's predictions

        # create csv file with first column is prediction, second column is key
        df_predictions = pd.DataFrame(predictions, columns=['predict'])
        compare = pd.concat([df_predictions, Y], axis=1, join='inner')
        compare.to_csv(path_or_buf=f'classified_{DATA_FILE}.csv', index=False)

        # calculate loss function: J_theta = 1/m * sigma((h_theta_n - y_n)**2)
        J_theta = 1 / m * np.sum(np.square(np.subtract(predictions, Y.to_numpy())))
        
        return J_theta


def main():
    # read data
    data = pd.read_csv(f'{DATA_FILE}.csv')   
    label_index = len(data.columns) - 1
    X = pd.DataFrame(data.iloc[:, :label_index])    # features
    Y = pd.DataFrame(data.iloc[:, label_index])     # classlabel

    # create linear regression model
    model = Regression(X, Y, ALPHA, NUM_EPOCHS)
    # output weights of regression model
    print('Weights:', model.thetas)
    # output mean squared error
    print('Mean Squared Error =', model.calculate_MSE(X, Y))

main()