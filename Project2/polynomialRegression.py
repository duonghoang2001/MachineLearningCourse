# Project 2: Linear and Polynomial Regression
# Part 2 & 3: Polynomial Regression using Basis Expansion and Plot Regression Lines
# Author: Duong Hoang
# CS 460G - 001
# Due Date: March 6th, 2022

'''
    Purpose: predict the synthetic data label using polynomial regression with
            gradient descent and plot regression lines of each model
    Pre-cond: a synthetic data csv file
    Post-cond: predictions generated by the polynomial regression with 
            full batch gradient descent using polynomial values of 2, 3, and 5; 
            the mean squared error averaged over the number of examples;
            the plot of each model's regression lines

'''

### Implementation ###


# initialize
DATA_FILE = 'synthetic-1'
ALPHA = 2                       # learning rate
NUM_EPOCHS = 50                 # iterations
NUM_ORDERS = 5                  # polynomial order

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


class Regression():
    def __init__(self, X: pd.DataFrame, Y: pd.DataFrame, orders: int, learning_rate: float, epochs: int):
        self.X = X              # features
        self.Y = Y              # classlabel
        self.orders = orders    # polynomial order
        self.expanded_X = self.basis_expansion(X) # expanded features
        self.thetas = self.coefficients(self.expanded_X, Y.to_numpy(),
                                learning_rate, epochs) # vector of coefficients
            
    
    def basis_expansion(self, X: pd.DataFrame):
        '''Return examples with expanded feature values using basis expansion'''
        
        expanded_X = X.to_numpy().copy()
        # expand feature values by number of orders
        for order in range(2, self.orders + 1):
            expanded_X = np.append(expanded_X, np.power(X, order), axis=1)

        return expanded_X


    def coefficients(self, X: np.matrix, Y: np.matrix, alpha: float, epochs: int):
        '''Return a vector of thetas using full batch update gradient descent'''
        
        # add the bias feature x_0 = 1 to examples
        X = np.insert(X, 0, [1] * Y.shape[0], axis=1) # X is a [m x n] matrix
        
        # initialize a list of thetas with random values
        m = X.shape[0] # batch size m = number of examples
        thetas = np.random.uniform(size=(X.shape[1], 1)) # [n x 1] matrix

        for epoch in range(epochs):
            # calculate h_theta =  sigma(theta_i * x_i)
            h_thetas = X.dot(thetas) # [m x n] [n x 1] = [m x 1]
            # calculate errors = predicted - expected = h_theta_i - y_i
            errors = np.subtract(h_thetas, Y) # [m x 1] - [m x 1] = [m x 1]
            # calculate update weight = (alpha * gradient) / m,
            # with gradient = error_i * x_i
            updates = (alpha / m) * X.transpose().dot(errors) # [n x m] [m x 1] = [n x 1]
            # update theta_i = theta_i - weight
            thetas -= updates # [n x 1] - [n x 1] = [n x 1]

        return thetas


    def linear_regression(self, X: pd.DataFrame):
        '''Return predictions of linear regression model'''

        # expand data
        expanded_X = self.basis_expansion(X)
        # add the bias feature x_0 = 1 to examples
        expanded_X = np.insert(expanded_X, 0, [1] * expanded_X.shape[0], axis=1) 
        # get model predictions h_theta = theta_n * x_n
        predictions = expanded_X.dot(self.thetas) 

        return predictions


    def loss_function(self, X: pd.DataFrame, Y: pd.DataFrame):
        '''Return the mean squared error of the model averaged over 
        the number of examples in the dataset
        '''
        
        m = len(Y) # number of examples        
        predictions = self.linear_regression(X) # linear regressor's predictions

        # create csv file with first column is prediction, second column is key
        df_predictions = pd.DataFrame(predictions)
        compare = pd.concat([df_predictions, Y], axis=1, join='inner')
        compare.to_csv(path_or_buf=f'classified_{DATA_FILE}.csv', index=False)

        # calculate cost function: J_theta = 1/m * sigma((h_theta_n - y_n)**2)
        J_theta = 1 / m * np.sum(np.square(np.subtract(predictions, Y.to_numpy())))
        
        return J_theta

    
    def visualize_model(self):
        '''Scatterplot visualization of examples and polynomial regression line'''

        # get regression model

        x = np.linspace(np.floor(self.X.min()), np.ceil(self.X.max()), 100)

        if self.orders == 2:
            y = self.thetas[0] + self.thetas[1] * x + self.thetas[2] * x**2
        elif self.orders == 3:
            y = self.thetas[0] + self.thetas[1] * x + self.thetas[2] * x**2 + self.thetas[3] * x**3
        else:
            y = self.thetas[0] + self.thetas[1] * x + self.thetas[2] * x**2 + self.thetas[3] * x**3 + self.thetas[4] * x**4 + self.thetas[5] * x**5
        

        # create a scatter plot of examples
        plt.scatter(x=self.X, y=self.Y, color='blue')
        plt.plot(x, y, color='red')
        plt.title(f'{DATA_FILE} {self.orders}-order Visualization')
        plt.xlabel('Feature Value')
        plt.ylabel('Class Label')

        plt.margins(x=0, y=0) # eliminate whitespace surround plot
        plt.savefig(f'visualize_{self.orders}-order_{DATA_FILE}.png', bbox_inches='tight')
        plt.show() # display plot and export to file


def main():
    # read data
    data = pd.read_csv(f'{DATA_FILE}.csv')   
    label_index = len(data.columns) - 1
    X = pd.DataFrame(data.iloc[:, :label_index])    # features
    Y = pd.DataFrame(data.iloc[:, label_index])     # classlabel

    # create linear regression model
    model = Regression(X, Y, NUM_ORDERS, ALPHA, NUM_EPOCHS)
    # output mean squared error
    print(model.loss_function(X, Y))
    model.visualize_model()

main()